
<html><head><title></title><link rel="stylesheet" type="text/css" href="defaultpage2.css"></head>

<body text="#ffffff" bgcolor="#666666" link="#ff9933" vlink="#ff9933" alink="#ff9933">

<font size="-2"><br><table width="1000" cellpadding="0" cellspacing="0" align="center"><tr>

<td valign="top"><font color="ffffff"><b>Publications</b> 
(<a href="FullPublication.html">Full Publication List</a><!-----; 
<A HREF="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Charlie_C=_L=.html" target="new">Entry@DBLP</A>; 
<A HREF="http://www.researcherid.com/rid/B-3730-2010" target="new">ResearcherID</A>; 
<A HREF="http://scholar.google.com/citations?user=-EXti64AAAAJ&hl=en" target="new">Google Scholar</A>--------->)<b>:</b> 
<a href="publication2005.html">2001-2005</a>; 
<a href="publication2010.html">2006-2010</a>; 
<a href="publication2015.html">2011-2015</a>; 
<a href="publication2020.html">2016-2020</a>; 
<a href="#2020">2020</a>; <a href="#2021">2021</a><!------ ; <a href="#UnderReview">Under Review</a>. ------>

<br><br>
<b>Disclaimer:</b> The documents listed on this page are copyright-protected. By clicking on the [PDF] links below, you confirm that you or your institution have the right to access the corresponding pdf file.

<br><br><b>Recent Papers</b>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" no="" resize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TMECHDefMannequin.jpg" width="160" border="0"></td><td width="830" valign="top">
Yingjun Tian, Guoxin Fang, Justas Petrulis, Andrew Weightman, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/TMECH.2022.3175759" target="new">Soft robotic mannequin: design and algorithm for deformation control</a>", IEEE/ASME Transactions on Mechatronics, Focused Section on TMECH/AIM Emerging Topics, accepted, May 2022. 
<a href="https://arxiv.org/abs/2205.05166" target="new">[arXiv]</a> 
<a href="https://youtu.be/5m2dzgiHc_8" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
This paper presents a novel soft robotic system for a deformable mannequin that can be employed to physically realize the 3D geometry of different human bodies. The soft membrane on a mannequin is deformed by inflating several curved chambers using pneumatic actuation. Controlling the freeform surface of a soft membrane by adjusting the pneumatic actuation in different chambers is challenging as the membrane's shape is commonly determined by interaction between all chambers. Using vision feedback provided by a structured-light based 3D scanner, we developed an efficient algorithm to compute the optimized actuation of all chambers which could drive the soft membrane to deform into the best approximation of different target shapes. Our algorithm converges quickly by including the step of pose estimation in the loop of optimization, and the time-consuming step for evaluating derivatives on the deformable membrane is avoided by using the Broyden update when possible. The effectiveness of our soft robotic mannequin with controlled deformation has been verified in experiments.
<p></p></td></tr>
</table>
<br><br>

<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TMECHJacoLearning.jpg" width="160" border="0"></td><td width="830" valign="top">
Guoxin Fang, Yingjun Tian, Zhi-Xin Yang, Jo M.P. Geraedts, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/TMECH.2022.3178303" target="new">Efficient Jacobian-based inverse kinematics with sim-to-real transfer of soft robots by learning</a>", IEEE/ASME Transactions on Mechatronics, accepted, May 2022. 
<a href="https://arxiv.org/abs/2012.13965" target="new">[arXiv]</a>
<a href="https://youtu.be/vv7u0SkDXCQ" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
This paper presents an efficient learning-based method to solve the <i>inverse kinematic</i> (IK) problem on soft robots with highly non-linear deformation. The major challenge of efficiently computing IK for such robots is due to the lack of analytical formulation for either forward or inverse kinematics. To address this challenge, we employ neural networks to learn both the mapping function of forward kinematics and also the Jacobian of this function. As a result, Jacobian-based iteration can be applied to solve the IK problem. A sim-to-real training transfer strategy is conducted to make this approach more practical. We first generate a large number of samples in a simulation environment for learning both the kinematic and the Jacobian networks of a soft robot design. Thereafter, a sim-to-real layer of differentiable neurons is employed to map the results of simulation to the physical hardware, where this sim-to-real layer can be learned from a very limited number of training samples generated on the hardware. The effectiveness of our approach has been verified on pneumatic-driven soft robots for path following and interactive positioning.
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/BAMRoboBioprinting.jpg" width="160" border="0"></td><td width="830" valign="top">
Zeyu Zhang, Chenming Wu, Chengkai Dai, Qingqing Shi, Guoxin Fang, Dongfang Xie, Yong-Jin Liu, <b>Charlie C.L. Wang</b>, and Xiu-Jie Wang, "<a href="retrieve/pii/S2452199X22000743.html" target="new">A multi-axis robot-based bioprinting platform for bioactive artificial blood vessel and cardiac tissue fabrication</a>", Bioactive Materials, vol.18, pp.138-150, December 2022. <a href="https://youtu.be/ymoosfEH0aw" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
Despite the recent advances in artificial tissue and organ engineering, how to generate large sizeviable and functional complex organs still remains as a grand challenge for regenerative medicine.Three-dimensional bioprinting has demonstrated its advantages as one of the major methods in 
fabricating simple tissues, yet it still faces difficulties to generate vasculatures and preserve somaticcell functions in complex organ production. Here, we overcome the limitations of conventionalbioprinting systems by converting a six degree-of-freedom robotic arm into a bioprinter, thereforeenables cell printing on 3D complex-shaped vascular scaffolds from all directions. We alsodeveloped an oil bath-based cell printing method to better preserve cell natural functions after 
printing. Together with a self-designed bioreactor and a repeated print-and-culture strategy, 
our bioprinting system is capable to generate vascularized, contractible, and long-term survived cardiactissues. Such bioprinting strategy mimics the in vivo organ development process and presents apromising solution for in vitro fabrication of complex organs.
<p></p></td></tr>
</table>
<br><br>

<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TOGHRBFFusion.jpg" width="160" border="0"></td><td width="830" valign="top">
Yabin Xu, Liangliang Nan, Laishui Zhou, Jun Wang, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1145/3516521" target="new">HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits</a>", ACM Transactions on Graphics, vol.41, no.3, article no.35 (19 pages), June 2022. 
<a href="pubs/TOGHRBFFusion.pdf" target="new">[PDF]</a> 
<a href="https://arxiv.org/abs/2202.01829" target="new">[arXiv]</a> 
<a href="https://github.com/YabinXuTUD/HRBFFusion3D" target="new">[Source Code]</a> 
<a href="https://youtu.be/ybhNYzMtSVo" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
Reconstruction of high-fidelity 3D objects or scenes is a fundamental research problem. Recent advances in RGB-D fusion have demonstrated the potential of producing 3D models from consumer-level RGB-D cameras. However, due to the discrete nature and limited resolution of their surface representations (e.g., point- or voxel-based), existing approaches suffer from the accumulation of errors in camera tracking and distortion in the reconstruction, which leads to an unsatisfactory 3D reconstruction. In this paper, we present a method using on-the-fly implicits of <i>Hermite Radial Basis Functions</i> (HRBFs) as a continuous surface representation for camera tracking in an existing RGB-D fusion framework. Furthermore, curvature estimation and confidence evaluation are coherently derived from the inherent surface properties of the on-the-fly HRBF implicits, which devote to a data fusion with better quality. We argue that our continuous but on-the-fly surface representation can effectively mitigate the impact of noise with its robustness and constrain the reconstruction with inherent surface smoothness when being compared with discrete representations. Experimental results on various real-world and synthetic datasets demonstrate that our HRBF-fusion outperforms the state-of-the-art approaches in terms of tracking robustness and reconstruction accuracy.
<p></p></td></tr>
</table>
<br><br>

<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/CAD3DHumanByCNN.jpg" width="160" border="0"></td><td width="830" valign="top">
Bin Liu, Xiuping Liu, Zhixin Yang, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1115/1.4054001" target="new">Concise and effective network for 3D human modeling from orthogonal silhouettes</a>", ASME Journal of Computing and Information Science in Engineering, vol.22, 051004  (11 pages), October 2022. 
<a href="https://arxiv.org/abs/1912.11616" target="new">[arXiv]</a> 
<a href="https://youtu.be/JEPAmiB0wYI" target="new">[Video@YouTube]</a> 
<a href="pubs/imageHuman3DUserStudy.pdf" target="new">[User Study]</a> 
<br><br>
<b>Abstract</b>
<br>
In this paper, we revisit the problem of 3D human modeling from two orthogonal silhouettes of individuals (i.e., front and side views). Different from our prior work, a supervised learning approach based on <i>convolutional neural network</i> (CNN) is investigated to solve the problem by establishing a mapping function that can effectively extract features from two silhouettes and fuse them into coefficients in the shape space of human bodies. A new CNN structure is proposed in our work to extract not only the discriminative features of front and side views and also their mixed features for the mapping function. 3D human models with high accuracy are synthesized from coefficients generated by the mapping function. Existing CNN approaches for 3D human modeling usually learn a large number of parameters (from 8M to 350M) from two binary images. Differently, we investigate a new network architecture and conduct the samples on silhouettes as input. As a consequence, more accurate models can be generated by our network with only 2.5M coefficients. The training of our network is conducted on samples obtained by augmenting a publicly accessible dataset. Learning transfer by using datasets with a smaller number of scanned models is applied to our network to enable the function of generating results with gender-oriented (or geographical) patterns.
<p></p></td></tr>
</table>
<br><br>

<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/AMIntTopoInterlock.jpg" width="160" border="0"></td><td width="830" valign="top">
Tim Kuipers, Renbo Su, Jun Wu, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1016/j.addma.2021.102495" target="new">ITIL: Interlaced topologically interlocking lattice for continuous dual-material extrusion</a>", Additive Manufacturing, vol.50, 102495 (11 pages), February 2022. 
<br><br>
<b>Abstract</b>
<br>
<!-----<I>Material Extrusion</I> (MEX) systems with dual-material capability can unlock interesting applications where flexible and rigid materials are combined. When chemically incompatible materials are concerned the adhesion between the two might be insufficient. Therefore researchers typically rely on dovetail type interlocking geometries in order to affix two bodies mechanically. However, dovetail type interlocking introduces extrusion discontinuities and relies on the material’s resistance to deformation, which is difficult to model. ---->
We propose a simple and effective 3D lattice consisting of interlaced horizontal beams in vertically alternating directions which interlock topologically: the <i>interlaced topologically interlocking lattice</i> (ITIL). It ensures continuous extrusion and ensures an interlock even for highly flexible materials. We develop analytical models for optimizing the ultimate tensile strength of the ITIL lattice in two different orientations relative to the interface: straight and diagonal. The analytical models are applied to <i>polypropylene</i> (PP) and <i>polylactic acid</i> (PLA) and verified by <i>finite elements method</i> (FEM) simulations and physical tensile experiments. In the diagonal orientation ITIL can obtain 82% of the theoretical upper bound of 8.6MPa. ITIL seems to perform comparably to dovetail interlocking designs, while it lends itself to application to non-vertical interfaces. Optimizing the lattice for non-vertical interfaces, however, remains future work.
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/AMToolpathCFRP.jpg" width="160" border="0"></td><td width="830" valign="top">
Xiangjia Chen, Guoxin Fang, Wei-Hsin Liao, and <b>Charlie C.L. Wang</b>, 
"<a href="https://doi.org/10.1016/j.addma.2021.102470" target="new">Field-based toolpath generation for 3D printing continuous fibre reinforced thermoplastic composites</a>", 
Additive Manufacturing, vol.49, 102470 (13 pages), January 2022. 
<a href="http://arxiv.org/abs/2112.12057" target="new">[arXiv]</a> 
<a href="https://github.com/SingaChen/CCFRP_DataSet" target="new">[Dataset of Examples]</a> 
<a href="https://youtu.be/EOqlirEFGbg" target="new">[Video@YouTube]</a>
<br><br>
<b>Abstract</b>
<br>
We present a field-based method of toolpath generation for 3D printing continuous fibre reinforced thermoplastic composites. Our method employs the strong anisotropic material property of continuous fibres by generating toolpaths along the directions of tensile stresses in the critical regions. Moreover, the density of toolpath distribution is controlled in an adaptive way proportionally to the values of stresses. Specifically, a vector field is generated from the stress tensors under given loads and processed to have better compatibility between neighboring vectors. An optimal scalar field is computed later by making its gradients approximate the vector field. After that, isocurves of the scalar field are extracted to generate the toolpaths for continuous fibre reinforcement, which are also integrated with the boundary conformal toolpaths in user selected regions. The performance of our method has been verified on a variety of models in different loading conditions. Experimental tests are conducted on specimens by 3D printing <i>continuous carbon fibres</i> (CCF) in a <i>polylactic acid</i> (PLA) matrix. Compared to reinforcement by load-independent toolpaths, the specimens fabricated by our method show up to 71.4% improvement on the mechanical strength in physical tests when using the same (or even slightly smaller) amount of continuous fibres.
<p></p></td></tr>
</table>
<br><br>



<a name="2021"></a>



<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/JCISELatticeImplicitModeling.jpg" width="160" border="0"></td><td width="830" valign="top">
Shengjun Liu, Tao Liu, Qiang Zou, Weiming Wang, Eugeni L. Doubrovski, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1115/1.4050290" target="new">Memory-efficient modeling and slicing of large-scale adaptive lattice structures</a>", ASME Journal of Computing and Information Science in Engineering, vol.21, no.6, 061003 (16 pages), December 2021. 
<a href="https://arxiv.org/abs/2101.05031" target="new">[arXiv]</a> 
<a href="Qiang-Zou/ImplicitSlicer.html" target="new">[Source Code]</a>  
<br><br>
<b>Abstract</b>
<br>
Lattice structures have been widely used in various applications of additive manufacturing due to its superior physical properties. If modeled by triangular meshes, a lattice structure with huge number of struts would consume massive memory. This hinders the use of lattice structures in large-scale applications (e.g., to design the interior structure of a solid with spatially graded material properties). To solve this issue, we propose a memory-efficient method for the modeling and slicing of adaptive lattice structures. A lattice structure is represented by a weighted graph where the edge weights store the struts' radii. When slicing the structure, its solid model is locally evaluated through convolution surfaces and in a streaming manner. As such, only limited memory is needed to generate the toolpaths of fabrication. Also, the use of convolution surfaces leads to natural blending at intersections of struts, which can avoid the stress concentration at these regions. We also present a computational framework for optimizing supporting structures and adapting lattice structures with prescribed density distributions. The presented methods have been validated by a series of case studies with large number (up to 100M) of struts to demonstrate its applicability to large-scale lattice structures.
<p></p></td></tr>
</table>
<br><br>

<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/CASE2021MotionPlanningMAAM.jpg" width="160" border="0"></td><td width="830" valign="top">
Tianyu Zhang, Xiangjia Chen, Guoxin Fang, Yingjun Tian, and <b>Charlie C.L. Wang</b>, "<a href="document/9462416/index.htm" target="new">Singularity-aware motion planning for multi-axis additive manufacturing</a>", IEEE Robotics and Automation Letters, 
Presented at IEEE International Conference on Automation Science and Engineering (CASE 2021), Lyon, France, August 23-27, 2021, vol.6, no.4, pp.6172-6179, October 2021. <b>(Finalist of Best Student Paper Award)</b>
<a href="http://arxiv.org/abs/2103.00273" target="new">[arXiv]</a> 
<a href="https://github.com/zhangty019/MultiAxis_3DP_MotionPlanning" target="new">[Source Code]</a> 
<a href="https://youtu.be/fsE2KpLk7ZI" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
Multi-axis additive manufacturing enables high flexibility of material deposition along dynamically varied directions. The Cartesian motion platforms of these machines include three parallel axes and two rotational axes. Singularity on rotational axes is a critical issue to be tackled in motion planning for ensuring high quality of manufacturing results. The highly nonlinear mapping in the singular region can convert a smooth toolpath with uniformly sampled waypoints defined in the model coordinate system into a highly discontinuous motion in the machine coordinate system, which leads to over-extrusion / under-extrusion of materials in filament-based additive manufacturing. The problem is challenging as both the maximal and the minimal speeds at the tip of a printer head must be controlled in motion. Moreover, collision may occur when sampling-based collision avoidance is employed. In this paper, we present a motion planning method to support the manufacturing realization of designed toolpaths for multi-axis additive manufacturing. Problems of singularity and collision are considered in an integrated manner to improve the motion therefore the quality of fabrication.
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/SIG2021Knitting4DGarment.jpg" width="160" border="0"></td><td width="830" valign="top">
Zishun Liu, Xingjian Han, Yuchen Zhang, Xiangjia Chen, Yukun Lai, Eugeni L. Doubrovski, Emily Whiting, and <b>Charlie C.L. Wang</b>, 
"<a href="doi/10.1145/3450626.3459868.html" target="new">Knitting 4D garment with elasticity controlled for body motion</a>", ACM Transactions on Graphics (SIGGRAPH 2021), vol.40, no.4, article no.62 (16 pages), August 2021. 
<a href="https://zishun.github.io/projects/Knitting4D" target="new">[Project]</a> 
<a href="pubs/SIG2021Knitting4DGarment.pdf" target="new">[PDF]</a> 
<a href="zishun/KnittingShortRows2021.html" target="new">[Source Code]</a> 
<a href="https://youtu.be/-AxtchabmIw" target="new">[Video@YouTube]</a>
<br><br>
<b>Abstract</b>
<br>
In this paper, we present a new computational pipeline for designing and fabricating 4D garments as knitwear that considers comfort during body movement. This is achieved by careful control of elasticity distribution to reduce uncomfortable pressure and unwanted sliding caused by body motion. We exploit the ability to knit patterns in different elastic levels by <i>single-jersey jacquard</i> (SJJ) with two yarns. We design the distribution of elasticity for a garment by physics-based computation, the optimized elasticity on the garment is then converted into instructions for a digital knitting machine by two algorithms proposed in this paper. Specifically, a graph-based algorithm is proposed to generate knittable stitch meshes that can accurately capture the 3D shape of a garment, and a tiling algorithm is employed to assign SJJ patterns on the stitch mesh to realize the designed distribution of elasticity. The effectiveness of our approach is verified on simulation results and on specimens physically fabricated by knitting machines.
</td>
<p></p></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TMECH3DSensing.jpg" width="160" border="0"></td><td width="830" valign="top">
Rob B.N. Scharff, Guoxin Fang, Yingjun Tian, Jun Wu, Jo M.P. Geraedts, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/TMECH.2021.3078263" target="new">Sensing and reconstruction of 3D deformation on pneumatic soft robots</a>", IEEE/ASME Transactions on Mechatronics, vol.26, no.4, pp.1877-1885, August 2021. 
<a href="https://arxiv.org/abs/2012.12411" target="new">[arXiv]</a> 
<a href="https://youtu.be/T9wqiIr3s-c" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
Real-time proprioception is a challenging problem for soft robots, which have almost infinite degrees-of-freedom in body deformation. When multiple actuators are used, it becomes more difficult as deformation can also occur on actuators caused by interaction between each other. To tackle this problem, we present a method in this paper to sense and reconstruct 3D deformation on pneumatic soft robots by first integrating multiple low-cost sensors inside the chambers of pneumatic actuators and then using machine learning to convert the captured signals into shape parameters of soft robots. An exterior motion capture system is employed to generate the datasets for both training and testing. With the help of good shape parameterization, the 3D shape of a soft robot can be accurately reconstructed from signals obtained from multiple sensors. We demonstrate the effectiveness of this approach on two designs of soft robots - a robotic joint and a deformable membrane. After parameterizing the deformation of these soft robots into compact shape parameters, we can effectively train the neural networks to reconstruct the 3D deformation from the sensor signals. The sensing and shape prediction pipeline can run at 50Hz in real-time on a consumer-level device.
</td>
<p></p></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/CIRP2021STLFreeSLM.jpg" width="160" border="0"></td><td width="830" valign="top">
Junhao Ding, Qiang Zou, Shuo Qu, Paulo Bartolo, Xu Song, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1016/j.cirp.2021.03.012" target="new">STL-free design and manufacturing paradigm for high-precision powder bed fusion</a>", CIRP Annals - Manufacturing Technology, vol.70, no.1, pp.167-170, July 2021. 
<a href="pubs/CIRP2021STLFreeSLM.pdf" target="new">[PDF]</a> 
<a href="Qiang-Zou/TPMSSlicer.html" target="new">[Source Code]</a>
<br><br>
<b>Abstract</b>
<br>
High-precision powder bed fusion (PBF), together with highly complex geometries necessitate a much more scalable representation of the geometry and an efficient computational pipeline. This paper presents a new digital design and manufacturing paradigm to solve the scalability and efficiency challenges by using the concept of STL-free workflow. It seamlessly integrates implicit solid modelling for design and direct slicing for manufacturing without any intermediate steps related to STL meshes. The presented paradigm has been validated by two case studies involving complex geometries filled with multiscale triply periodic minimal surfaces (TPMS), which are fabricated by PBF with laser beam size 25&#956;m.
</td>
<p></p></tr>
</table>
<br><br>

<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TASEDepthImageCNNCompletion.jpg" width="161" border="0"></td><td width="830" valign="top">
Chuhua Xian, Dongjiu Zhang, Chengkai Dai, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/TASE.2020.3002069" target="new">Fast generation of high fidelity RGB-D images by deep-learning with adaptive convolution</a>", IEEE Transactions on Automation Science and Engineering, vol.18, no.3, pp.1328-1340, July 2021. 
<a href="https://arxiv.org/abs/2002.05067" target="new">[arXiv]</a> 
<a href="chuhuaxian/HF-RGBD.html" target="new">[Source Code & Dataset]</a> 
<br><br>
<b>Abstract</b>
<br>
Using the raw data from consumer-level RGB-D cameras as input, we propose a deep-learning based approach to efficiently generate RGB-D images with completed information in high resolution. To process the input images in low resolution with missing regions, new operators for adaptive convolution are introduced in our deep-learning network that consists of three cascaded modules - the completion module, the refinement module and the super-resolution module. The completion module is based on an architecture of encoder-decoder, where the features of input raw RGB-D will be automatically extracted by the encoding layers of a deep neural-network. The decoding layers are applied to reconstruct the completed depth map, which is followed by a refinement module to sharpen the boundary of different regions. For the super-resolution module, we generate RGB-D images in high resolution by multiple layers for feature extraction and a layer for up-sampling. Benefited from the adaptive convolution operators newly proposed in this paper, our results outperform the existing deep-learning based approaches for RGB-D image complete and super-resolution. As an end-to-end approach, high fidelity RGB-D images can be generated efficiently at the rate of around 21 frames per second.
<p></p></td></tr>
</table>
<br><br>


<a name="2020"></a>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/SIGAsia2020ReinforcedFDM.jpg" width="161" border="0"></td><td width="830" valign="top">
Guoxin Fang, Tianyu Zhang, Sikai Zhong, Xiangjia Chen, Zichun Zhong, and <b>Charlie C.L. Wang</b>, "<a href="doi/10.1145/3414685.3417834.html" target="new">Reinforced FDM:
Multi-axis filament alignment with controlled anisotropic strength</a>", ACM Transactions on Graphics (SIGGRAPH Asia 2020), 
vol.39, no.6, article no.204 (15 pages), November 2020. 
<a href="https://guoxinfang.github.io/ReinforcedFDM" target="new">[Project]</a> 
<a href="pubs/SIGAsia2020ReinforcedFDM.pdf" target="new">[PDF]</a> 
<a href="GuoxinFang/ReinforcedFDM.html" target="new">[Source Code]</a> 
<a href="https://youtu.be/X2o2-SJFv2M" target="new">[Video@YouTube]</a>
<br><br>
<b>Abstract</b>
<br>
The anisotropy of mechanical strength on a 3D printed model can be controlled in a multi-axis 3D printing system as materials can be accumulated along dynamically varied directions. In this paper, we present a new computational framework to generate specially designed layers and toolpaths of multi-axis 3D printing for strengthening a model by aligning filaments along the directions with large stresses. The major challenge comes from how to effectively decompose a solid into a sequence of strength-aware and collision-free working surfaces. We formulate it as a problem to compute an optimized governing field together with a selected orientation of fabrication setup. Iso-surfaces of the governing field are extracted as working surface layers for filament alignment. Supporting structures in curved layers are constructed by extrapolating the governing field to enable the fabrication of overhangs. Compared with planar-layer based <i>Fused Deposition Modeling</i> (FDM) technology, models fabricated by our method can withstand up to 6.35x loads in experimental tests.
<p></p></td></tr>
</table>
<br><br>



<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/CAD3DHumanSparseRep.jpg" width="160" border="0"></td><td width="830" valign="top">
Yiu-Bun Wu, Bin Liu, Xiuping Liu, and <b>Charlie C.L. Wang</b>, 
"<a href="https://doi.org/10.1016/j.cad.2020.102913" target="new">Data-driven human modeling by sparse representation</a>", 
Computer-Aided Design, vol.128, 102913, November 2020. <a href="pubs/CAD3DHumanSparseRep.pdf" target="new">[PDF]</a>
<br><br>
<b>Abstract</b>
<br>
Data-driven methods for modeling the realistic shape of 3D human bodies need to access datasets that contain a large amount of 3D human models. A very challenging problem is to find an appropriate representation for storing these 3D models as their raw data representations in triangular meshes take a large amount of space. We develop a method based on sparse representation in this paper to represent 3D human models as signals of patches. Unlike the general mesh compression approaches, all mesh models used in a data-driven human modeling framework have the same mesh connectivity. By using this property, we segment a human model into patches containing the same number of vertices. L0-learning algorithm is selected to train an overcomplete dictionary matrix, which in turn introduced sparse representation of the dataset. Patch signals of individual human models can then be extracted by using the dictionary matrix. With the ease of balance control between sparsity and accuracy that is featured by the chosen learning algorithm, a representation with high compression ratio and low shape-approximation error can be determined. The results have been compared with the widely used statistic representation based on <i>principal component analysis</i> (PCA) to verify the effectiveness of our approach. Moreover, the method for using sparse representation in the regression-based statistical modeling of 3D human models has been presented at the end of the paper.
<p></p></td></tr>
</table>
<br><br>




<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/CADContourParaToolpath.jpg" width="160" border="0"></td><td width="830" valign="top">
Tim Kuipers, Eugeni L. Doubrovski, Jun Wu, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1016/j.cad.2020.102907" target="new">A framework for adaptive width control of dense contour-parallel toolpaths in fused deposition modeling</a>", Computer-Aided Design, vol.128, 102907, November 2020. 
<a href="https://arxiv.org/abs/2004.13497" target="new">[arXiv]</a> 
<br><br>
<b>Abstract</b>
<br>
3D printing techniques such as Fused Deposition Modeling (FDM) have enabled the fabrication of complex geometry quickly and cheaply. High stiffness parts are produced by filling the 2D polygons of consecutive layers with contour-parallel extrusion toolpaths. Uniform width toolpaths consisting of inward offsets from the outline polygons produce over- and underfill regions in the center of the shape, which are especially detrimental to the mechanical performance of thin parts. In order to fill shapes with arbitrary diameter densely the toolpaths require adaptive width. Existing approaches for generating toolpaths with adaptive width result in a large variation in widths, which for some hardware systems is difficult to realize accurately. In this paper we present a framework which supports multiple schemes to generate toolpaths with adaptive width, by employing a function to decide the number of beads and their widths. Furthermore, we propose a novel scheme which reduces extreme bead widths, while limiting the number of altered toolpaths. We statistically validate the effectiveness of our framework and this novel scheme on a data set of representative 3D models, and physically validate it by developing a technique, called back pressure compensation, for off-the-shelf FDM systems to effectively realize adaptive width.
<p></p></td></tr>
</table>
<br><br>



<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TASETrajectoryPlanning.jpg" width="160" border="0"></td><td width="830" valign="top">
Chengkai Dai, Sylvain Lefebvre, Kai-Ming Yu, Jo M.P. Geraedts, and <b>Charlie C.L. Wang</b>, 
"<a href="https://doi.org/10.1109/TASE.2020.2974771" target="new">Planning jerk-optimized trajectory with discrete-time constraints for redundant robots</a>", IEEE Transactions on Automation Science and Engineering, vol.17, no.4, pp.1711-1724, October 2020. 
<a href="https://arxiv.org/abs/1909.06570" target="new">[arXiv]</a> 
<a href="https://youtu.be/e8ISmh9MPrE" target="new">[Video@YouTube]</a> <br><br>
<b>Abstract</b>
<br>
We present a method for effectively planning the motion trajectory of robots in manufacturing tasks, the tool-paths of which are usually complex and have a large number of discrete-time constraints as waypoints. Kinematic redundancy also exists in these robotic systems. The jerk of motion is optimized in our trajectory planning method at the meanwhile of fabrication process to improve the quality of fabrication. Our method is based on a sampling strategy and consists of two major parts. After determining an initial path by graph-search, a greedy algorithm is adopted to optimize a path by locally applying adaptive filers in the regions with large jerks. The filtering result is obtained by numerical optimization. In order to achieve efficient computation, an adaptive sampling method is developed for learning a collision-indication function that is represented as a support-vector machine. Applications in robot-assisted 3D printing are given in this paper to demonstrate the functionality of our approach.
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/CASELearningDecomp3DP.jpg" width="160" border="0"></td><td width="830" valign="top">
Chenming Wu, Yong-Jin Liu, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/LRA.2020.3011369" target="new">Learning to accelerate decomposition for multi-directional 3D printing</a>", IEEE Robotics and Automation Letters, Presented at IEEE International Conference on Automation Science and Engineering (CASE 2020), Hong Kong, August 20-24, 2020, vol.5, no.4, pp.5897-5904, October 2020. 
<a href="https://arxiv.org/abs/2004.03450" target="new">[arXiv]</a> 
<a href="https://github.com/chenming-wu/pymdp/" target="new">[Source Code]</a> 
<a href="https://youtu.be/OOkxNKGhN7A" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b>
<br>
Multi-directional 3D printing has the capability of decreasing or eliminating the need for support structures. 
Recent work proposed a beam-guided search algorithm to find an optimized sequence of plane-clipping, which gives volume decomposition of a given 3D model. 
Different printing directions are employed in different regions to fabricate a model with tremendously less support (or even no support in many cases). 
To obtain optimized decomposition, a large beam width needs to be used in the search algorithm, leading to a very time-consuming computation. 
In this paper, we propose a learning framework that can accelerate the beam-guided search by using a smaller number of the original beam width to obtain results with similar quality. 
Specifically, we use the results of beam-guided search with large beam width to train a scoring function for candidate clipping planes based on six newly proposed feature metrics. 
With the help of these feature metrics, both the current and the sequence-dependent information are captured by the neural network to score candidates of clipping. 
As a result, we can achieve around 3x computational speed. We test and demonstrate our accelerated decomposition on a large dataset of models for 3D printing.
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TROSoftKinematics.jpg" width="160" border="0">
</td>
<td width="830" valign="top">
Guoxin Fang, Christopher-Denny Matte, Rob B.N. Scharff, Tsz-Ho Kwok, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/TRO.2020.2985583" target="new">Kinematics of soft robots by geometric computing</a>", IEEE Transactions on Robotics, vol.36, no.4, pp.1272-1286, August 2020. 
<a href="pubs/TROSoftKinematics.pdf" target="new">[PDF]</a> 
<a href="GuoxinFang/SoftRobotKinematics.html" target="new">[Source Code]</a> 
<a href="https://youtu.be/RTc0wWSmDFw" target="new">[Video@YouTube]</a> 

<br><br>

(This is an extended version of the paper - <a href="https://doi.org/10.1109/ICRA.2018.8461088" target="new">Geometry-based direct simulation for multi-material soft robots</a>, 
which is published in 2018 IEEE ICRA Conference, Brisbane, Australia, May 21-25, 2018.) 
<!--------- <a href="https://youtu.be/vTKMGV1uf_c" target="new">[Video@YouTube]</a>)  ----->
<br><br>
<b>Abstract</b><br>
Robots fabricated with soft materials can provide higher flexibility and thus better safety while interacting in unpredictable situations. However, the usage of soft material makes it challenging to predict the deformation of a continuum body under actuation and therefore brings difficulty to the kinematic control of its movement. In this paper, we present a geometry-based framework for computing the deformation of soft robots within the range of linear material elasticity. After formulating both manipulators and actuators with geometry elements, deformation can be efficiently computed by solving a constrained optimization problem. Based on its efficiency, forward and inverse kinematics for soft manipulators can be effectively solved by an iterative algorithm. Meanwhile, components with multiple materials can also be geometrically modeled in our framework with the help of a simple calibration. Numerical and physical experimental tests are conducted on soft manipulators driven by different actuators with large deformation to demonstrate the performance of our approach.
<br>
<p></p></td></tr>
</table>
<br><br>



<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/RoboSoft2020ForceSensingControl.jpg" width="160" border="0">
</td>
<td width="830" valign="top">
Alice Buso, Rob B.N. Scharff, Eugeni L. Doubrovski, Jun Wu, <b>Charlie C.L. Wang</b>, and Peter Vink, 
"Soft robotic module for sensing and controlling contact force", 
IEEE International Conference on Soft Robotics (RoboSoft 2020), Yale University, New Haven, Connecticut, USA, April 6-9, 2020. 
<a href="pubs/RoboSoft2020ForceSensingControl.pdf" target="new">[PDF]</a> <a href="https://youtu.be/kVt9N1XT10A" target="new">[Video@YouTube]</a> <br><br>
<b>Abstract</b><br>
This work presents a soft robotic module that can sense and control contact forces. The module is composed of a foam spring encapsulated by a pneumatic bellow that can be inflated to increase its stiffness. Optical sensors and a light source are integrated inside the soft pneumatic module. Changes in shape of the module lead to a variation in light reflectivity, which is captured by the optical sensors. These shape measurements are combined with air pressure measurements to predict the contact force through a machine learning model. Using these predictions, a closed-loop control of the contact force was implemented. The modules can be applied to realize pressure distribution control in support devices such as seats and mattresses. The presented method is robust and low-cost, can measure both shape and contact force, and does not require (rigid) sensors to be present at the movable contact interface between the support device and the user.
<br>
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/TASEMultiDir3DP.jpg" width="160" border="0">
</td>
<td width="830" valign="top">
Chenming Wu, Chengkai Dai, Guoxin Fang, Yong-Jin Liu, and <b>Charlie C.L. Wang</b>, "<a href="https://doi.org/10.1109/TASE.2019.2938219" target="new">General support-effective decomposition for multi-directional 3-D printing</a>", IEEE Transactions on Automation Science and Engineering, vol.17, no.2, pp.599-610, April 2020. 
<a href="https://arxiv.org/abs/1812.00606" target="new">[arXiv]</a> 
<a href="https://github.com/chenming-wu/pymdp/" target="new">[Source Code]</a> 
<a href="https://youtu.be/8SOxeFh9SDo" target="new">[Video@YouTube]</a> 
<a href="https://3dprint.com/231922/researchers-decrease-support-structures-for-models-through-multidirectional-3d-printing/" target="new">[Report@3DPrint.com]</a>
<br><br>
<b>Abstract</b><br>
We present a method for fabricating general models with multi-directional 3D printing systems by printing different model regions along different directions. The core of our method is a support-effective volume decomposition algorithm that minimizes the area of the regions with large overhangs. A beam-guided searching algorithm with manufacturing constraints determines the optimal volume decomposition, which is represented by a sequence of clipping planes. While current approaches require manually assembling separate components into a final model, our algorithm allows for directly printing the final model in a single pass. It can also be applied to models with loops and handles. A supplementary algorithm generates special supporting structures for models where supporting structures for large overhangs cannot be eliminated. We verify the effectiveness of our method using two hardware systems: a Cartesian-motion based system and an angular-motion based system. A variety of 3-D models have been successfully fabricated on these systems.
<br>
<p></p></td></tr>
</table>
<br><br>



<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/SMOSpaceTimeTopoOpt.jpg" width="160" border="0">
</td>
<td width="830" valign="top">
Weiming Wang, Dirk Munro, <b>Charlie C.L. Wang</b>, Fred van Keulen, and Jun Wu, 
"<a href="https://doi.org/10.1007/s00158-019-02420-6" target="new">Space-time topology optimization for additive manufacturing: concurrent optimization of structural layout and fabrication sequence</a>", Structural and Multidisciplinary Optimization, vol.61, pp.1-18, January 2020. <b>(ISSMO/Springer Prize)</b>  

<a href="http://homepage.tudelft.nl/z0s1z/projects/2019-space-time-topology-optimization.html" target="new">[Project]</a> 
<a href="pubs/SMOSpaceTimeTopoOpt.pdf" target="new">[PDF]</a> 
<br><br>
<b>Abstract</b><br>
The design of optimal structures and the planning of (additive manufacturing) fabrication sequences have been considered typically as two separate tasks that are performed consecutively. In the light of recent advances in robot-assisted (wire-arc) additive manufacturing which enable addition of material along curved surfaces, we present a novel topology optimization formulation which concurrently optimizes the structure and the fabrication sequence. For this, two sets of design variables, i.e. a density field for defining the structural layout, and a time field which determines the fabrication process order, are simultaneously optimized. These two fields allow to generate a sequence of intermediate structures, upon which manufacturing constraints (e.g. fabrication continuity and speed) are imposed. The proposed space-time formulation is general, and is demonstrated on three fabrication settings, considering self-weight of the intermediate structures, process-dependent critical loads, and time-dependent material properties.
<br>
<p></p></td></tr>
</table>
<br><br>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/MDBonding3DP.jpg" width="160" border="0">
</td>
<td width="830" valign="top">
Lars Rossing, Rob B.N. Scharff, Bryan Chompff, <b>Charlie C.L. Wang</b>, and Eugeni L. Doubrovski, "<a href="https://doi.org/10.1016/j.matdes.2019.108254" target="new">Bonding between silicones and thermoplastics using 3D printed mechanical interlocking</a>", Materials & Design, vol.186, article no.108254, January 2020. 
<a href="https://youtu.be/tGKluNrPQAU" target="new">[Video@YouTube]</a> 
<br><br>
<b>Abstract</b><br>
Silicones have desirable properties such as skin-safety, high temperature-resistance, and flexibility. Many applications require the presence of a hard body connected to the silicone. Traditionally, it has been difficult to create strong bonding between silicones and hard materials. In this study, a technique is presented to control the bonding strength between silicones and thermoplastics through mechanical interlocking. This is realized through a hybrid fabrication method where silicone is cast onto a 3D-printed mold and structure. The influence of the structure's design parameters on the bonding strength is explored through theoretical modeling and physical testing while the manufacturability of the 3D-printed structure is ensured. A CAD tool is developed to automatically apply the bonding structure to product surfaces. The user interface visualizes the theoretical strength of the cells as the designer adjusts the cell parameters, allowing the designer to iteratively optimize the structure to the product's load case. The bonding strength of the presented mechanical interlocking structure is more than 5.5 times higher than can be achieved with a commercially available primer. The presented technique enables custom digital design and manufacturing of durable free-form parts. This is demonstrated through application of the technique in over-molded products, airtight seals, and soft pneumatic actuators.
<br>
<p></p></td></tr>
</table>
<br><br>







<b>Book Chapter and Survey Papers</b>


<br><br>
<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize=""></td><td valign="top" align="left" width="170">
<img src="pubs/AMSurveyML.jpg" width="160" border="0"></td>
<td width="830" valign="top">
Jian Qin, Fu Hu, Ying Liu, Paul Witherell, <b>Charlie C.L. Wang</b>, David W. Rosen, Timothy Simpson, Yan Lu, Qian Tang, "<a href="https://doi.org/10.1016/j.addma.2022.102691" target="new">Research and application of machine learning for additive manufacturing</a>", Additive Manufacturing, vol.52, 102691 (25 pages), April 2022. 
<br><br>
<b>Abstract</b>
<br>
Additive manufacturing (AM) is poised to bring a revolution due to its unique production paradigm. It offers the prospect of mass customization, flexible production, on-demand and decentralized manufacturing. However, a number of challenges stem from not only the complexity of manufacturing systems but the demand for increasingly complex and high-quality products, in terms of design principles, standardization and quality control. These challenges build up barriers to the widespread adoption of AM in the industry and the in-depth research of AM in academia. To tackle the challenges, machine learning (ML) technologies rise to play a critical role as they are able to provide effective ways to quality control, process optimization, modelling of complex systems, and energy management. Hence, this paper employs a systematic literature review method as it is a defined and methodical way of identifying, assessing, and analysing published literature. Then, a keyword co-occurrence and cluster analysis are employed for analysing relevant literature. Several aspects of AM, including Design for AM (DfAM), material analytics, in situ monitoring and defect detection, property prediction and sustainability, have been clustered and summarized to present state-of-the-art research in the scope of ML for AM. Finally, the challenges and opportunities of ML for AM are uncovered and discussed.
<p></p></td></tr>
</table>
<br><br>


<!-------------


<a name="UnderReview"></a>

<B>Under Review</B>
<BR><BR>


<table width="1000" border="0" cellpadding="0" cellspacing="0" tableborder="0"><tr>
<td align="center" valing="top" noresize><td valign=top align=left width=170>
<img src="pubs/RALCollisionAwareGeoSim.jpg" width="160" border="0"></td><td width=830 valign=top>
Guoxin Fang, Yingjun Tian, Andrew Weightman, and <B>Charlie C.L. Wang</B>, "<a href="https://arxiv.org/abs/2203.02054" target="new">Collision-aware fast simulation for soft robots by optimization-based geometric computing</a>", under revision, 2022. 
<a href="https://youtu.be/DRwwh5kO4io" target="new">[Video@YouTube]</a> 
<BR><BR>
<B>Abstract</B>
<BR>
Soft robots are able to safely interact with environments because of their mechanical compliance. Self-collision is also employed in the modern design of soft robots to enhance their performance in different tasks. However, developing an efficient and reliable simulator which can handle the collision response well, is still a challenging task in the research of soft robotics. This paper presents a collision-aware simulator based on geometric optimization, in which we develop a highly efficient and realistic collision checking / response model incorporating a hyperelastic material property. Both actuated deformation and collision response for soft robots are formulated as geometry-based objectives. The collision-free body of a soft robot can be obtained by minimizing the geometry-based objective function. Different from the FEA-based physical simulation, the proposed pipeline performs a much lower computational cost. Moreover, adaptive remeshing is applied to achieve the improvement of the convergence when dealing with soft robots having large volume variations. Experimental tests are conducted on different soft robots to verify the performance of our approach.
<p></p></tr>
</table>
<BR><BR>

--------->


</font></td></tr><tr><td colspan="3" align="center"><p><br><p><br></p></td></tr></table>
<br><br> 

</font></body></html>